---
layout: page
title: CV
permalink: /cv/
image: '/images/cv/CV_intro.gif'
---

><cite>üî•Machine Learning Courseüî•
<br>Computer Vision ‚Äì From OpenCV to SOTA</cite>

><cite>Ch√†o m·ªçi ng∆∞·ªùi. M√¨nh l√† Minh. ƒê√¢y l√† b√†i vi·∫øt b√†i vi·∫øt ƒë·∫ßu ti√™n c·ªßa m√¨nh trong chu·ªói b√†i v·ªÅ Th·ªã gi√°c m√°y t√≠nh. M√¨nh d√†nh th·ªùi gian vi·∫øt chu·ªói b√†i n√†y ƒë·ªÉ gi·ªõi thi·ªáu t·ªõi m·ªçi ng∆∞·ªùi nh·ªØng ki·∫øn th·ª©c nh·ªè m√† m√¨nh h·ªçc ƒë∆∞·ª£c. Hy v·ªçng n√≥ s·∫Ω gi√∫p √≠ch v·ªõi m·ªçi ng∆∞·ªùi.</cite>

### Part I: From OpenCV to Convolutional Neural Network for Computer Vision
<details>
  <summary>Table of Content</summary>

  <ol>
    <li><a href="#about-the-project">What is Computer Vision?</a> </li>
    <li><a href="#usage">Computer Vision and Applications</a></li>
    <li><a href="#roadmap">Understanding Images</a></li>
    <li><a href="#contributing">Images and Colors</a></li>
    <li><a href="#license">Classifying Images based on Features</a></li>
    <li><a href="#contact">Image Filters</a></li>
    <li><a href="#acknowledgments">Face Detection</a></li>
    <li><a href="#acknowledgments">Image Features</a></li>
    <li><a href="#acknowledgments">Convolutional Neural Networks</a></li>
  </ol>
</details>


#### Applications of Computer Vision
C√°c ·ª©ng d·ª•ng c·ªßa th·ªã gi√°c m√°y t√≠nh bao g·ªìm t·ª± ƒë·ªông h√≥a trong c√°c ph∆∞∆°ng ti·ªán t·ª± l√°i ƒë·∫øn ph√°t tri·ªÉn ph·∫ßn m·ªÅm nh·∫≠n d·∫°ng ch√≠nh x√°c ƒë·ªÉ ki·ªÉm tra s·∫£n ph·∫©m trong d√¢y chuy·ªÅn s·∫£n xu·∫•t s·∫£n xu·∫•t ƒë·∫øn ƒëi·ªÅu khi·ªÉn robot, t·ªï ch·ª©c th√¥ng tin, ch·∫≥ng h·∫°n nh∆∞ l·∫≠p ch·ªâ m·ª•c c∆° s·ªü d·ªØ li·ªáu h√¨nh ·∫£nh.

Th·ªã gi√°c m√°y t√≠nh, c≈©ng nh∆∞ c√°c kh√°i ni·ªám v·ªÅ AI v√† h·ªçc m√°y, l√† ch√¨a kh√≥a ƒë·ªÉ hi·ªán th·ª±c h√≥a qu√° tr√¨nh t·ª± ƒë·ªông h√≥a ho√†n ch·ªânh, ho·∫∑c C·∫•p ƒë·ªô 5 trong c√°c ph∆∞∆°ng ti·ªán t·ª± l√°i. T·∫°i ƒë√¢y ph·∫ßn m·ªÅm CV ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ c√°c camera ƒë·∫∑t xung quanh xe. ƒêi·ªÅu n√†y cho ph√©p chi·∫øc xe x√°c ƒë·ªãnh c√°c ph∆∞∆°ng ti·ªán kh√°c v√† ng∆∞·ªùi ƒëi b·ªô c≈©ng nh∆∞ ƒë·ªçc c√°c bi·ªÉn b√°o ƒë∆∞·ªùng b·ªô.
Th·ªã gi√°c m√°y t√≠nh c≈©ng l√† ch√¨a kh√≥a ƒë·ªÉ ph√°t tri·ªÉn ph·∫ßn m·ªÅm nh·∫≠n d·∫°ng khu√¥n m·∫∑t ch√≠nh x√°c. ƒêi·ªÅu n√†y th∆∞·ªùng xuy√™n ƒë∆∞·ª£c c√°c c∆° quan th·ª±c thi ph√°p lu·∫≠t th·ª±c hi·ªán c≈©ng nh∆∞ gi√∫p x√°c th·ª±c quy·ªÅn s·ªü h·ªØu thi·∫øt b·ªã c·ªßa ng∆∞·ªùi ti√™u d√πng.

C√¥ng ngh·ªá th·ª±c t·∫ø tƒÉng c∆∞·ªùng v√† h·ªón h·ª£p ƒëang ng√†y c√†ng ƒë∆∞·ª£c tri·ªÉn khai tr√™n ƒëi·ªán tho·∫°i th√¥ng minh v√† m√°y t√≠nh b·∫£ng. Hay nh∆∞ smart glass c≈©ng ƒëang tr·ªü n√™n ph·ªï bi·∫øn r·ªông r√£i h∆°n.
T·∫•t c·∫£ nh·ªØng ƒëi·ªÅu n√†y ƒë√≤i h·ªèi th·ªã gi√°c c·ªßa m√°y t√≠nh ƒë·ªÉ gi√∫p x√°c ƒë·ªãnh v·ªã tr√≠, ph√°t hi·ªán c√°c ƒë·ªëi t∆∞·ª£ng v√† thi·∫øt l·∫≠p ƒë·ªô s√¢u ho·∫∑c k√≠ch th∆∞·ªõc c·ªßa th·∫ø gi·ªõi ·∫£o.	
Trong khu√¥n kh·ªï b√†i vi·∫øt n√†y, d·ªØ li·ªáu ƒë∆∞·ª£c m√¨nh s·ª≠ d·ª•ng ng·∫´u nhi√™n, do ƒë√≥ n·∫øu b·∫°n mu·ªën th·ª≠, h√£y s·ª≠ d·ª•ng d·ªØ li·ªáu c·ªßa b·∫°n. Hay th·ª≠ tinh ch·ªâ code c·ªßa m√¨nh v·ªõi d·ªØ li·ªáu c·ªßa b·∫°n ƒë·ªÉ xem c√≥ g√¨ b·∫•t ng·ªù x·∫£y ra nh√© üòä
><cite> B√†i vi·∫øt n√†y y√™u c·∫ßu k·ªπ nƒÉng l·∫≠p tr√¨nh v·ªõi ng√¥n ng·ªØ python ·ªü m·ª©c ƒë·ªô c∆° b·∫£n/ trung b√¨nh. Let‚Äôs get it started.</cite>

### Understanding Images.
OpenCV (CV2) l√† m·ªôt th∆∞ vi·ªán c·ª±c k·ª≥ n·ªïi ti·∫øng cho c√°c ·ª©ng d·ª•ng th·ªã gi√°c m√°y t√≠nh. B·∫°n c√≥ th·ªÉ xem m√£ ngu·ªìn v√† [h∆∞·ªõng d·∫´n t·∫°i ƒë√¢y n√®](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)<br>

[NumPy](https://numpy.org/doc/) l√† m·ªôt th∆∞ vi·ªán ph·ª•c v·ª• cho khoa h·ªçc m√°y t√≠nh c·ªßa Python, h·ªó tr·ª£ cho vi·ªác t√≠nh to√°n c√°c m·∫£ng nhi·ªÅu chi·ªÅu, c√≥ k√≠ch th∆∞·ªõc l·ªõn v·ªõi c√°c h√†m ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u √°p d·ª•ng l√™n c√°c m·∫£ng nhi·ªÅu chi·ªÅu ƒë√≥. Numpy ƒë·∫∑c bi·ªát h·ªØu √≠ch khi th·ª±c hi·ªán c√°c h√†m li√™n quan t·ªõi ƒê·∫°i S·ªë Tuy·∫øn T√≠nh.

```
import numpy as np
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import cv2

%matplotlib inline

# Read in the image
image = mpimg.imread('images/oranges.jpg')

# Print out the image dimensions
print('Image dimensions:', image.shape)
plt.imshow(image)
```


<img src="/images/cv/1.png">

```
# Change from color to grayscale
gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

plt.imshow(gray_image, cmap='gray')
```


<img src="/images/cv/2.png">

```
 
# Specific grayscale pixel values
# Pixel value at x = 400 and y = 300 

x = 200
y = 100

print(gray_image[y,x])12
>>> 6
# 5x5 image using just grayscale, numerical values
tiny_image = np.array([[0, 20, 30, 150, 120],
                      [200, 200, 250, 70, 3],
                      [50, 180, 85, 40, 90],
                      [240, 100, 50, 255, 10],
                      [30, 0, 75, 190, 220]])

# To show the pixel grid, use matshow
plt.matshow(tiny_image, cmap='gray')
```


<img src="/images/cv/3.png">

Images and Colors
```
In [7]:
# Read in the image
image = mpimg.imread('images/rainbow_flag.jpg')

plt.imshow(image)
```


<img src="/images/cv/4.png">

```
RGB Channels
In [8]:
# Isolate RGB channels
r = image[:,:,0]
g = image[:,:,1]
b = image[:,:,2]

# The individual color channels
f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,10))
ax1.set_title('R channel')
ax1.imshow(r, cmap='gray')
ax2.set_title('G channel')
ax2.imshow(g, cmap='gray')
ax3.set_title('B channel')
ax3.imshow(b, cmap='gray')
```


<img src="/images/cv/5.png">

Color Threshold
```
IIMG_PATH='introcv/'
image = cv2.imread(IMG_PATH+'images/pizza_bluescreen.jpg')
print('This image is:', type(image), ' with dimensions:', image.shape)

>>> This image is: <class 'numpy.ndarray'>  with dimensions: (514, 816, 3)

image_copy = np.copy(image)

# RGB (from BGR)
image_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)

# Display the image copy
plt.imshow(image_copy)
``` 


<img src="/images/cv/6.png">

```
# Color Threshold
lower_blue = np.array([0,0,200]) 
upper_blue = np.array([50,50,255])

```

Mask
```
# Define the masked area

mask = cv2.inRange(image_copy, lower_blue, upper_blue)

# Vizualize the mask

plt.imshow(mask, cmap='gray')
```
 

<img src="/images/cv/7.png">

```
# Masking the image to let the pizza show through

masked_image = np.copy(image_copy)

masked_image[mask != 0] = [0, 0, 0]

plt.imshow(masked_image)
```


<img src="/images/cv/8.png">

```
# Loading in a background image, and converting it to RGB 

background_image = cv2.imread(IMG_PATH+'images/space_background.jpg')

background_image = cv2.cvtColor(background_image, cv2.COLOR_BGR2RGB)

# Cropping it to the right size (514x816)

crop_background = background_image[0:514, 0:816]

# Masking the cropped background so that the pizza area is blocked

crop_background[mask == 0] = [0, 0, 0]

# Displaying the background

plt.imshow(crop_background)
```


<img src="/images/cv/9.png">

```
# Adding the two images together to create a complete image!
complete_image = masked_image + crop_background

# Displaying the result
plt.imshow(complete_image)
 ```
 
 
<img src="/images/cv/10.png">
 
HSV
```
 image = cv2.imread(IMG_PATH+'images/water_balloons.jpg')

image_copy = np.copy(image)

image = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)

plt.imshow(image)
```



<img src="/images/cv/11.png">

```
# Converting from RGB to HSV
hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)

# HSV channels
h = hsv[:,:,0]
s = hsv[:,:,1]
v = hsv[:,:,2]

f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,10))

ax1.set_title('Hue')
ax1.imshow(h, cmap='gray')

ax2.set_title('Saturation')
ax2.imshow(s, cmap='gray')

ax3.set_title('Value')
ax3.imshow(v, cmap='gray')
```



<img src="/images/cv/12.png">

```
 
# Color selection criteria in HSV values for getting only Pink balloons
lower_hue = np.array([160,0,0]) 
upper_hue = np.array([180,255,255])

# Defining the masked area in HSV space
mask_hsv = cv2.inRange(hsv, lower_hue, upper_hue)

# masking the image
masked_image = np.copy(image)
masked_image[mask_hsv==0] = [0,0,0]

# Vizualizing the mask
plt.imshow(masked_image)
```



<img src="/images/cv/13.png">

Classifying Images based on Features
```
In [21]:
# Helper function
import glob # h√†m gi√∫p load image t·ª´ directory

# This function loads in images and their labels and places them in a list
# The list contains all images and their associated labels
# For example, after data is loaded, im_list[0][:] will be the first image-label pair in the list

def load_dataset(image_dir):
    
    # Populate this empty image list
    im_list = []
    image_types = ["day", "night"]
    
    # Iterate through each color folder
    for im_type in image_types:
        
        # Iterate through each image file in each image_type folder
        # glob reads in any image with the extension "image_dir/im_type/*"
        for file in glob.glob(os.path.join(image_dir, im_type, "*")):
            
            # Read in the image
            im = mpimg.imread(file)
            
            # Check if the image exists/if it's been correctly read-in
            if not im is None:
          	# Append the image, and it's type (red, green, yellow) to the imgae list
                im_list.append((im, im_type))
    
    return im_list


```
```
## Standardizing the input images
# Resizing each image to the desired input size: 600x1100px (hxw).

## Standardizing the output
# With each loaded image, we also specify the expected output.
# For this, we use binary numerical values 0/1 = night/day.


# This function should take in an RGB image and return a new, standardized version
# 600 height x 1100 width image size (px x px)
def standardize_input(image):
    
    # Resize image and pre-process so that all "standard" images are the same size
    standard_im = cv2.resize(image, (1100, 600))
    
    return standard_im


# Examples:
# encode("day") should return: 1
# encode("night") should return: 0
def encode(label):
    
    numerical_val = 0
    if(label == 'day'):
        numerical_val = 1
    # else it is night and can stay 0
    
    return numerical_val

# using both functions above, standardize the input images and output labels
def standardize(image_list):
    
    # Empty image data array
    standard_list = []
    
    # Iterate through all the image-label pairs
    for item in image_list:
        image = item[0]
        label = item[1]
        
        # Standardize the image
        standardized_im = standardize_input(image)
        
        # Create a numerical label
        binary_label = encode(label)
        
        # Append the image, and it's one hot encoded label to the full, processed list of image data
        standard_list.append((standardized_im, binary_label))
    
    return standard_list

```
#### Day and Night Image Classifier
B·ªô d·ªØ li·ªáu h√¨nh ·∫£nh ng√†y/ƒë√™m bao g·ªìm 200 h√¨nh ·∫£nh m√†u RGB. M·ªói v√≠ d·ª• c√≥ s·ªë l∆∞·ª£ng b·∫±ng nhau: 100 h√¨nh ·∫£nh ng√†y v√† 100 h√¨nh ·∫£nh ban ƒë√™m.</br>
X√¢y d·ª±ng m·ªôt c√¥ng c·ª• ph√¢n lo·∫°i c√≥ th·ªÉ g·∫Øn nh√£n ch√≠nh x√°c nh·ªØng h√¨nh ·∫£nh n√†y l√† ng√†y hay ƒë√™m v√† ƒëi·ªÅu ƒë√≥ d·ª±a v√†o vi·ªác t√¨m ra c√°c ƒë·∫∑c ƒëi·ªÉm ph√¢n bi·ªát gi·ªØa hai lo·∫°i h√¨nh ·∫£nh!<br>
Note: data is here: [AMOS dataset<br>](http://mvrl.cs.uky.edu/datasets/amos/)
Training and Testing Data<br>
Chia th√†nh t·∫≠p training v√† testing
‚Ä¢	60% l√† traning
‚Ä¢	40% l√† testing 
```
image_dir_training = "introcv/day_night_images/training/"
image_dir_test = "introcv/day_night_images/test/"

# Load training data

IMAGE_LIST = load_dataset(image_dir_training)


Visualizing an Image
image_index = 20
selected_image = IMAGE_LIST[image_index][0]
selected_label = IMAGE_LIST[image_index][1]

print(len(IMAGE_LIST))
print(selected_image.shape)

plt.imshow(selected_image)
```


<img src="/images/cv/14.png">

```
 
Preprocessed images with labels
STANDARDIZED_LIST = standardize(IMAGE_LIST)
image_num = 0
selected_image = STANDARDIZED_LIST[image_num][0]
selected_label = STANDARDIZED_LIST[image_num][1]

# show ·∫£nh
plt.imshow(selected_image)
print("Shape: "+str(selected_image.shape))
print("Label [1 = day, 0 = night]: " + str(selected_label))
```


<img src="/images/cv/15.png">


Feature Extraction

```
# Chuy·ªÉn sang kh√¥ng gian m√†u HSV
# M√¥ t·∫£ c√°c k√™nh m√†u ri√™ng l·∫ª

image_num = 0
test_im = STANDARDIZED_LIST[image_num][0]
test_label = STANDARDIZED_LIST[image_num][1]

hsv = cv2.cvtColor(test_im, cv2.COLOR_RGB2HSV)

print('Label: ' + str(test_label))

# HSV channels
h = hsv[:,:,0]
s = hsv[:,:,1]
v = hsv[:,:,2]

# V·∫Ω ·∫£nh g·ªëc v√† 3 k√™nh
f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,10))
ax1.set_title('Standardized image')
ax1.imshow(test_im)
ax2.set_title('H channel')
ax2.imshow(h, cmap='gray')
ax3.set_title('S channel')
ax3.imshow(s, cmap='gray')
ax4.set_title('V channel')
ax4.imshow(v, cmap='gray')
```


<img src="/images/cv/16.png">

```
def avg_brightness(rgb_image):
    hsv = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)

    # C·ªông t·∫•t c·∫£ c√°c gi√° tr·ªã pixel trong 3 k√™nh
    sum_brightness = np.sum(hsv[:,:,2])
    area = 600*1100.0  # pixels
    
    avg = sum_brightness/area
    
    return avg

# test gi√° tr·ªã s√°ng trung b√¨nh

image_num = 190
test_im = STANDARDIZED_LIST[image_num][0]

avg = avg_brightness(test_im)
print('Avg brightness: ' + str(avg))
plt.imshow(test_im)
```


<img src="/images/cv/17.png">

 
Classifier
```
# This function should take in RGB image input
def estimate_label(rgb_image):
    
    # Extracting average brightness feature from an RGB image 
    avg = avg_brightness(rgb_image)
        
    # Using the avg brightness feature to predict a label (0, 1)
    predicted_label = 0
    threshold = 98
    if(avg > threshold):
        # if the average brightness is above the threshold value, we classify it as "day"
        predicted_label = 1
    # else, the pred-cted_label can stay 0 (it is predicted to be "night")
    
    return predicted_label    
    
```

##### Testing Classifier
```
import random

# Load test data
TEST_IMAGE_LIST = load_dataset(image_dir_test)

# Standardize the test data
STANDARDIZED_TEST_LIST = standardize(TEST_IMAGE_LIST)

# Shuffle the standardized test data
random.shuffle(STANDARDIZED_TEST_LIST)
```


Determining Accuracy
```
def get_misclassified_images(test_images):
    # Tracking misclassified images by placing them into a list
    misclassified_images_labels = []

    # Iterating through all the test images
    for image in test_images:

        im = image[0]
        true_label = image[1]

        predicted_label = estimate_label(im)

        # Comparing true and predicted labels 
        if(predicted_label != true_label):
            # If these labels are not equal, the image has been misclassified
            misclassified_images_labels.append((im, predicted_label, true_label))
            
    return misclassified_images_labels

MISCLASSIFIED = get_misclassified_images(STANDARDIZED_TEST_LIST)

# Accuracy calculations
total = len(STANDARDIZED_TEST_LIST)
num_correct = total - len(MISCLASSIFIED)
accuracy = num_correct/total

print('Accuracy: ' + str(accuracy))
print("Number of misclassified images = " + str(len(MISCLASSIFIED)) +' out of '+ str(total))
```


>>> Accuracy: 0.91875
>>> Number of misclassified images = 13 out of 160


### Image Filters
#### Fourier Transforms
C√°c th√†nh ph·∫ßn t·∫ßn s·ªë c·ªßa h√¨nh ·∫£nh c√≥ th·ªÉ ƒë∆∞·ª£c hi·ªÉn th·ªã sau khi th·ª±c hi·ªán Bi·∫øn ƒë·ªïi Fourier (FT). FT xem x√©t c√°c th√†nh ph·∫ßn c·ªßa h√¨nh ·∫£nh (c√°c c·∫°nh c√≥ t·∫ßn s·ªë cao v√† c√°c v√πng c√≥ m√†u m·ªãn l√† t·∫ßn s·ªë th·∫•p) v√† v·∫Ω bi·ªÉu ƒë·ªì t·∫ßn s·ªë xu·∫•t hi·ªán d∆∞·ªõi d·∫°ng c√°c ƒëi·ªÉm trong quang ph·ªï.<br>
Tr√™n th·ª±c t·∫ø, FT coi c√°c m·∫´u c∆∞·ªùng ƒë·ªô trong h√¨nh ·∫£nh l√† s√≥ng h√¨nh sin v·ªõi m·ªôt t·∫ßn s·ªë c·ª• th·ªÉ v√† b·∫°n c√≥ th·ªÉ xem h√¨nh ·∫£nh tr·ª±c quan th√∫ v·ªã v·ªÅ c√°c th√†nh ph·∫ßn s√≥ng h√¨nh sin n√†y [·ªü ƒë√¢y.](https://plus.maths.org/content/fourier-transforms-images)<br>
Fourier Transform l√† m·ªôt c√¥ng c·ª• x·ª≠ l√Ω h√¨nh ·∫£nh quan tr·ªçng ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ph√¢n r√£ h√¨nh ·∫£nh th√†nh c√°c th√†nh ph·∫ßn sin v√† cosine c·ªßa n√≥. ƒê·∫ßu ra c·ªßa ph√©p bi·∫øn ƒë·ªïi ƒë·∫°i di·ªán cho h√¨nh ·∫£nh trong mi·ªÅn Fourier ho·∫∑c mi·ªÅn t·∫ßn s·ªë, trong khi h√¨nh ·∫£nh ƒë·∫ßu v√†o l√† mi·ªÅn kh√¥ng gian t∆∞∆°ng ƒë∆∞∆°ng. Trong ·∫£nh mi·ªÅn Fourier, m·ªói ƒëi·ªÉm bi·ªÉu di·ªÖn m·ªôt t·∫ßn s·ªë c·ª• th·ªÉ c√≥ trong ·∫£nh mi·ªÅn kh√¥ng gian.<br>
Fourier Transform ƒë∆∞·ª£c s·ª≠ d·ª•ng trong m·ªôt lo·∫°t c√°c ·ª©ng d·ª•ng, ch·∫≥ng h·∫°n nh∆∞ ph√¢n t√≠ch h√¨nh ·∫£nh, l·ªçc h√¨nh ·∫£nh, t√°i t·∫°o h√¨nh ·∫£nh v√† n√©n h√¨nh ·∫£nh.<br>
V√† m·ªôt ch√∫t to√°n h·ªçc ·ªü ƒë√¢y: 
[Image Transforms - Fourier Transform](https://homepages.inf.ed.ac.uk/rbf/HIPR2/fourier.htm)
[But what is the Fourier Transform? A visual introduction.](https://www.youtube.com/watch?v=spUNpyF58BY)

```
image_stripes = cv2.imread(IMG_PATH+'images/stripes.jpg')
image_stripes = cv2.cvtColor(image_stripes, cv2.COLOR_BGR2RGB)

image_solid = cv2.imread(IMG_PATH+'images/pink_solid.jpg')
image_solid = cv2.cvtColor(image_solid, cv2.COLOR_BGR2RGB)
```


<img src="/images/cv/17.png">

# Displaying
```
f, (ax1,ax2) = plt.subplots(1, 2, figsize=(10,5))

ax1.imshow(image_stripes)
ax2.imshow(image_solid)
 
# chuy·ªÉn ƒë·ªïi sang thang ƒë·ªô x√°m ƒë·ªÉ t·∫≠p trung v√†o c√°c m·∫´u c∆∞·ªùng ƒë·ªô trong h√¨nh ·∫£nh
gray_stripes = cv2.cvtColor(image_stripes, cv2.COLOR_RGB2GRAY)
gray_solid = cv2.cvtColor(image_solid, cv2.COLOR_RGB2GRAY)

# chu·∫©n h√≥a c√°c gi√° tr·ªã m√†u c·ªßa h√¨nh ·∫£nh trong ph·∫°m vi t·ª´ [0,255] ƒë·∫øn [0,1] 

norm_stripes = gray_stripes/255.0
norm_solid = gray_solid/255.0

# th·ª±c hi·ªán bi·∫øn ƒë·ªïi fourier nhanh 
# v√† t·∫°o h√¨nh ·∫£nh bi·∫øn ƒë·ªïi t·∫ßn s·ªë ƒë∆∞·ª£c chia t·ª∑ l·ªá

def ft_image(norm_image):
    '''This function takes in a normalized, grayscale image
       and returns a frequency spectrum transform of that image. '''
    f = np.fft.fft2(norm_image)
    fshift = np.fft.fftshift(f)
    frequency_tx = 20*np.log(np.abs(fshift))
    
    return frequency_tx

f_stripes = ft_image(norm_stripes)
f_solid = ft_image(norm_solid)

# displaying the images
# ·∫£nh g·ªëc ·ªü b√™n tr√°i bi·∫øn ƒë·ªïi t·∫ßn s·ªë c·ªßa ch√∫ng

f, (ax1,ax2,ax3,ax4) = plt.subplots(1, 4, figsize=(20,10))

ax1.set_title('original image')
ax1.imshow(image_stripes)
ax2.set_title('frequency transform image')
ax2.imshow(f_stripes, cmap='gray')

ax3.set_title('original image')
ax3.imshow(image_solid)
ax4.set_title('frequency transform image')
ax4.imshow(f_solid, cmap='gray')
```


<img src="/images/cv/19.png">

```
image = cv2.imread(IMG_PATH+'images/brain_MR.jpg')

image_copy = np.copy(image)

image_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)

plt.imshow(image_copy)
```


<img src="/images/cv/20.png">


 
# Converting to grayscale for filtering

```
gray = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)

# Creating a Gaussian blurred image
gray_blur = cv2.GaussianBlur(gray, (9, 9), 0)

f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))

ax1.set_title('original gray')
ax1.imshow(gray, cmap='gray')

ax2.set_title('blurred image')
ax2.imshow(gray_blur, cmap='gray')
```


<img src="/images/cv/21.png">



# High-pass filter 


```
# 3x3 sobel filters for edge detection
sobel_x = np.array([[ -1, 0, 1], 
                   [ -2, 0, 2], 
                   [ -1, 0, 1]])


sobel_y = np.array([[ -1, -2, -1], 
                   [ 0, 0, 0], 
                   [ 1, 2, 1]])


# Filters the orginal and blurred grayscale images using filter2D
filtered = cv2.filter2D(gray, -1, sobel_x)

filtered_blurred = cv2.filter2D(gray_blur, -1, sobel_y)
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))

ax1.set_title('original gray')
ax1.imshow(filtered, cmap='gray')

ax2.set_title('blurred image')
ax2.imshow(filtered_blurred, cmap='gray')
```


<img src="/images/cv/22.png">

```
retval, binary_image = cv2.threshold(filtered_blurred, 30, 255, cv2.THRESH_BINAR)

plt.imshow(binary_image, cmap='gray')
```


<img src="/images/cv/23.png">
 
 
 
### Face Detection
M·ªôt ch∆∞∆°ng tr√¨nh c≈©, nh∆∞ng v·∫´n ph·ªï bi·∫øn ƒë·ªÉ ph√°t hi·ªán khu√¥n m·∫∑t l√† b·ªô ph√¢n lo·∫°i t·∫ßng Haar; c√°c b·ªô ph√¢n lo·∫°i n√†y trong th∆∞ vi·ªán OpenCV v√† s·ª≠ d·ª•ng c√°c t·∫ßng ph√¢n lo·∫°i d·ª±a tr√™n t√≠nh nƒÉng h·ªçc c√°ch c√¥ l·∫≠p v√† ph√°t hi·ªán c√°c khu√¥n m·∫∑t trong m·ªôt h√¨nh ·∫£nh. b√†i b√°o g·ªëc ƒë·ªÅ xu·∫•t c√°ch ti·∫øp c·∫≠n n√†y [·ªü ƒë√¢y.](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf)
V√† ·ªü ƒë√¢y: [OpenCV: Cascade Classifier](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)

```
# loading in color image for face detection
image = cv2.imread(IMG_PATH+'images/multi_faces.jpg')

# converting to RBG
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

plt.figure(figsize=(20,10))
plt.imshow(image)
```


<img src="/images/cv/24.png">

```
 
# converting to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  

plt.figure(figsize=(20,10))
plt.imshow(gray, cmap='gray')
```
 
 
<img src="/images/cv/25.png">
 
 
 
L∆∞u √Ω ·ªü c√°c tham s·ªë.
C√≥ nhi·ªÅu khu√¥n m·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán ƒë∆∞·ª£c x√°c ƒë·ªãnh b·ªüi ch·ª©c nƒÉng detectorMultiScale nh·∫±m m·ª•c ƒë√≠ch ph√°t hi·ªán c√°c khu√¥n m·∫∑t c√≥ k√≠ch th∆∞·ªõc kh√°c nhau. C√°c ƒë·∫ßu v√†o cho ch·ª©c nƒÉng n√†y l√†: (image, scaleFactor, minNeighbors); b·∫°n th∆∞·ªùng s·∫Ω ph√°t hi·ªán nhi·ªÅu khu√¥n m·∫∑t h∆°n v·ªõi scaleFactor nh·ªè h∆°n v√† gi√° tr·ªã minNeighbors th·∫•p h∆°n, nh∆∞ng vi·ªác n√¢ng cao c√°c gi√° tr·ªã n√†y th∆∞·ªùng t·∫°o ra c√°c k·∫øt qu·∫£ kh·ªõp t·ªët h∆°n. S·ª≠a ƒë·ªïi c√°c gi√° tr·ªã n√†y t√πy thu·ªôc v√†o h√¨nh ·∫£nh ƒë·∫ßu v√†o c·ªßa b·∫°n.
```
# loading in cascade classifier
face_cascade = cv2.CascadeClassifier('introcv/detector_architectures/haarcascade_frontalface_default.xml')

# running the detector on the grayscale image
faces = face_cascade.detectMultiScale(gray, 4, 6)\

# printing out the detections found
print ('We found ' + str(len(faces)) + ' faces in this image')
print ("Their coordinates and lengths/widths are as follows")
print ('=============================')
print (faces)
```


<img src="/images/cv/26.png">

```

# m·ªôt b·∫£n sao c·ªßa h√¨nh ·∫£nh g·ªëc ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì ph√°t hi·ªán h√¨nh ch·ªØ nh·∫≠t
img_with_detections = np.copy(image)   

# l·∫∑p l·∫°i c√°c ph√°t hi·ªán c·ªßa v√† v·∫Ω c√°c box t∆∞∆°ng ·ª©ng c·ªßa l√™n tr√™n h√¨nh ·∫£nh ban ƒë·∫ßu
for (x,y,w,h) in faces:  

    cv2.rectangle(img_with_detections,(x,y),(x+w,y+h),(255,0,0),5)  

plt.figure(figsize=(20,10))
plt.imshow(img_with_detections)
```


<img src="/images/cv/27.png">


 
### Image Features
#### Harris Corner Detection
Harris Corner Detection l√† m·ªôt thu·∫≠t to√°n ph√°t hi·ªán g√≥c th∆∞·ªùng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c thu·∫≠t to√°n th·ªã gi√°c m√°y t√≠nh ƒë·ªÉ tr√≠ch xu·∫•t c√°c g√≥c v√† suy ra c√°c ƒë·∫∑c ƒëi·ªÉm c·ªßa h√¨nh ·∫£nh. N√≥ ƒë∆∞·ª£c gi·ªõi thi·ªáu l·∫ßn ƒë·∫ßu ti√™n b·ªüi Chris Harris v√† Mike Stephens v√†o nƒÉm 1988 sau khi c·∫£i ti·∫øn m√°y d√≤ g√≥c c·ªßa Moravec. So v·ªõi tr∆∞·ªõc ƒë√≥, m√°y d√≤ g√≥c c·ªßa Harris t√≠nh ƒë·∫øn s·ª± kh√°c bi·ªát c·ªßa ƒëi·ªÉm g√≥c v·ªõi tham chi·∫øu tr·ª±c ti·∫øp ƒë·∫øn h∆∞·ªõng, thay v√¨ s·ª≠ d·ª•ng c√°c b·∫£n v√° d·ªãch chuy·ªÉn cho m·ªói g√≥c 45 ƒë·ªô v√† ƒë√£ ƒë∆∞·ª£c ch·ª©ng minh l√† ch√≠nh x√°c h∆°n trong vi·ªác ph√¢n bi·ªát gi·ªØa c√°c c·∫°nh v√† g√≥c . K·ªÉ t·ª´ ƒë√≥, n√≥ ƒë√£ ƒë∆∞·ª£c c·∫£i ti·∫øn v√† √°p d·ª•ng trong nhi·ªÅu thu·∫≠t to√°n ƒë·ªÉ x·ª≠ l√Ω tr∆∞·ªõc h√¨nh ·∫£nh cho c√°c ·ª©ng d·ª•ng ti·∫øp theo.

```
# Read in the image
image = cv2.imread(IMG_PATH+'images/waffle.jpg')

# Make a copy of the image
image_copy = np.copy(image)

# Change color to RGB (from BGR)
image_copy = cv2.cvtColor(image_copy, cv2.COLOR_BGR2RGB)

plt.imshow(image_copy)
```


<img src="/images/cv/28.png">


```
 
gray = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)
gray = np.float32(gray)

# Detecting corners 
dst = cv2.cornerHarris(gray, 2, 3, 0.04)

# Dilating corner image to enhance corner points
dst = cv2.dilate(dst,None)

plt.imshow(dst, cmap='gray')
```


<img src="/images/cv/29.png">

```
 
thresh = 0.1*dst.max()

# Creating an image copy to draw corners on
corner_image = np.copy(image_copy)

# Iterating through all the corners and draw them on the image (if they pass the threshold)
for j in range(0, dst.shape[0]):
    for i in range(0, dst.shape[1]):
        if(dst[j,i] > thresh):
            # image, center pt, radius, color, thickness
            cv2.circle( corner_image, (i, j), 1, (0,255,0), 1)

plt.imshow(corner_image)
```


<img src="/images/cv/30.png">



#### Contour Detection
M·ªói contour ƒë·ªÅu c√≥ m·ªôt s·ªë ƒë·∫∑c ƒëi·ªÉm c√≥ th·ªÉ ƒë∆∞·ª£c t√≠nh to√°n, bao g·ªìm di·ªán t√≠ch c·ªßa contour, h∆∞·ªõng c·ªßa n√≥ (h∆∞·ªõng m√† h·∫ßu h·∫øt contour h∆∞·ªõng v√†o), chu vi v√† nhi·ªÅu thu·ªôc t√≠nh kh√°c ƒë∆∞·ª£c n√™u trong [OpenCV documentation.](https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html)

```
image = cv2.imread(IMG_PATH+'images/thumbs_up_down.jpg')

image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

plt.imshow(image)
```


<img src="/images/cv/31.png">

```
gray = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)

# Binary thresholded image
retval, binary = cv2.threshold(gray, 225, 255, cv2.THRESH_BINARY_INV)

plt.imshow(binary, cmap='gray')
```


<img src="/images/cv/32.png">

```
 
# Finding contours from thresholded, binary image
contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

# Drawing all contours on a copy of the original image
contours_image = np.copy(image)
contours_image = cv2.drawContours(contours_image, contours, -1, (0,255,0), 3)

plt.imshow(contours_image)
```


<img src="/images/cv/33.png">


#### K-Means Clustering
Ph√¢n c·ª•m K-mean l√† m·ªôt ph∆∞∆°ng ph√°p l∆∞·ª£ng t·ª≠ h√≥a vect∆°, ban ƒë·∫ßu t·ª´ x·ª≠ l√Ω t√≠n hi·ªáu, nh·∫±m m·ª•c ƒë√≠ch ph√¢n chia n quan s√°t th√†nh k c·ª•m trong ƒë√≥ m·ªói quan s√°t thu·ªôc v·ªÅ c·ª•m c√≥ gi√° tr·ªã trung b√¨nh g·∫ßn nh·∫•t (cluster centers ho·∫∑c cluster centroid), ƒë√≥ng vai tr√≤ l√† nguy√™n m·∫´u c·ªßa c·ª•m. ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn vi·ªác ph√¢n v√πng kh√¥ng gian d·ªØ li·ªáu th√†nh c√°c √¥ Voronoi (Voronoi cells). Ph√¢n c·ª•m K-mean gi·∫£m thi·ªÉu c√°c ph∆∞∆°ng sai trong c·ª•m (b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch Euclide), nh∆∞ng kh√¥ng ph·∫£i kho·∫£ng c√°ch Euclid th√¥ng th∆∞·ªùng, ƒë√¢y s·∫Ω l√† b√†i to√°n Weber kh√≥ h∆°n: gi√° tr·ªã trung b√¨nh t·ªëi ∆∞u h√≥a sai s·ªë b√¨nh ph∆∞∆°ng, trong khi ch·ªâ c√≥ trung v·ªã h√¨nh h·ªçc gi·∫£m thi·ªÉu kho·∫£ng c√°ch Euclid. V√≠ d·ª•, c√°c gi·∫£i ph√°p Euclid t·ªët h∆°n c√≥ th·ªÉ ƒë∆∞·ª£c t√¨m th·∫•y b·∫±ng c√°ch s·ª≠ d·ª•ng k-medians v√† k-medoid.
·ª®ng d·ª•ng c·ªßa thu·∫≠t to√°n K-mean r·∫•t nhi·ªÅu, trong ƒë√≥ c√≥ vi·ªác n√©n dung l∆∞·ª£ng ·∫£nh m√† kh√¥ng l√†m m·∫•t ƒëi qu√° nhi·ªÅu ch·∫•t l∆∞·ª£ng ·∫£nh (image compression), feature learning, cluster analysis, vector quantization...
```
image = cv2.imread(IMG_PATH+'images/monarch.jpg')

image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

plt.imshow(image)
```


<img src="/images/cv/34.png">

```
 
# Reshaping image into a 2D array of pixels and 3 color values (RGB)
pixel_vals = image.reshape((-1,3))

# Converting to float type
pixel_vals = np.float32(pixel_vals)
# you can change the number of max iterations for faster convergence!
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 0.2)

k = 3

retval, labels, centers = cv2.kmeans(pixel_vals, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)

# converting data into 8-bit values
centers = np.uint8(centers)
segmented_data = centers[labels.flatten()]

# reshaping data into the original image dimensions
segmented_image = segmented_data.reshape((image.shape))
labels_reshape = labels.reshape(image.shape[0], image.shape[1])

plt.imshow(segmented_image)
```


<img src="/images/cv/35.png">


Image Pyramids
Image Pyramids (pyramid - Kim t·ª± th√°p), l√† m·ªôt lo·∫°i bi·ªÉu di·ªÖn t√≠n hi·ªáu ƒëa t·ª∑ l·ªá ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi c·ªông ƒë·ªìng x·ª≠ l√Ω t√≠n hi·ªáu, x·ª≠ l√Ω h√¨nh ·∫£nh v√† th·ªã gi√°c m√°y t√≠nh, trong ƒë√≥ m·ªôt t√≠n hi·ªáu ho·∫∑c h√¨nh ·∫£nh ph·∫£i ƒë∆∞·ª£c l√†m m·ªãn v√† l·∫•y m·∫´u con l·∫∑p l·∫°i. Bi·ªÉu di·ªÖn kim t·ª± th√°p l√† ti·ªÅn th√¢n c·ªßa bi·ªÉu di·ªÖn kh√¥ng gian quy m√¥ v√† ph√¢n t√≠ch ƒëa gi·∫£i.


```
image = cv2.imread(IMG_PATH+'images/rainbow_flag.jpg')

image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

plt.imshow(image)
```


<img src="/images/cv/36.png">

```
 
level_1 = cv2.pyrDown(image)
level_2 = cv2.pyrDown(level_1)
level_3 = cv2.pyrDown(level_2)

# Displaying the images
f, (ax1,ax2,ax3,ax4) = plt.subplots(1, 4, figsize=(20,10))

ax1.set_title('original')
ax1.imshow(image)

ax2.imshow(level_1)
ax2.set_xlim([0, image.shape[1]])
ax2.set_ylim([0, image.shape[0]])

ax3.imshow(level_2)
ax3.set_xlim([0, image.shape[1]])
ax3.set_ylim([0, image.shape[0]])

ax4.imshow(level_3)
ax4.set_xlim([0, image.shape[1]])
ax4.set_ylim([0, image.shape[0]])
```



<img src="/images/cv/37.png">


#### Convolutional Neural Networks
Trong deep learning, m·∫°ng n∆°-ron ph·ª©c h·ª£p (CNN, ho·∫∑c ConvNet) l√† m·ªôt l·ªõp m·∫°ng n∆°-ron nh√¢n t·∫°o, ƒë∆∞·ª£c √°p d·ª•ng ph·ªï bi·∫øn nh·∫•t ƒë·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh tr·ª±c quan. Ch√∫ng c√≤n ƒë∆∞·ª£c g·ªçi l√† m·∫°ng n∆°-ron nh√¢n t·∫°o b·∫•t bi·∫øn ho·∫∑c b·∫•t bi·∫øn trong kh√¥ng gian (SIANN), d·ª±a tr√™n ki·∫øn tr√∫c tr·ªçng s·ªë chia s·∫ª c·ªßa c√°c nh√¢n ho·∫∑c b·ªô l·ªçc t√≠ch ch·∫≠p tr∆∞·ª£t d·ªçc theo c√°c t√≠nh nƒÉng ƒë·∫ßu v√†o v√† cung c·∫•p c√°c ph·∫£n h·ªìi t∆∞∆°ng ƒë∆∞∆°ng d·ªãch ƒë∆∞·ª£c g·ªçi l√† b·∫£n ƒë·ªì ƒë·ªëi t∆∞·ª£ng. V·ªÅ m·∫∑t ph·∫£n tr·ª±c gi√°c, h·∫ßu h·∫øt c√°c m·∫°ng n∆°-ron t√≠ch ch·∫≠p ch·ªâ t∆∞∆°ng ƒë∆∞∆°ng, tr√°i ng∆∞·ª£c v·ªõi b·∫•t bi·∫øn, ƒë·ªëi v·ªõi ph√©p d·ªãch. H·ªç c√≥ c√°c ·ª©ng d·ª•ng trong nh·∫≠n d·∫°ng h√¨nh ·∫£nh v√† video, h·ªá th·ªëng khuy·∫øn ngh·ªã, ph√¢n lo·∫°i h√¨nh ·∫£nh, ph√¢n ƒëo·∫°n h√¨nh ·∫£nh, ph√¢n t√≠ch h√¨nh ·∫£nh y t·∫ø, x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, v√† chu·ªói th·ªùi gian t√†i ch√≠nh‚Ä¶

```
img_path = IMG_PATH+'images/car.png'

bgr_img = cv2.imread(img_path)
gray_img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2GRAY)

gray_img = gray_img.astype("float32")/255

plt.imshow(gray_img, cmap='gray')
plt.show()
```


<img src="/images/cv/38.png">

Filters

```
M·ªôt v√†i filter th∆∞·ªùng x·ª≠ d·ª•ng ƒë·ªÉ bi·∫øn ƒë·ªïi h√¨nh ·∫£nh qua ph√©p t√≠ch ch·∫≠p
filter_vals = np.array([[-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1], [-1, -1, 1, 1]])
# defining four filters
filter_1 = filter_vals
filter_2 = -filter_1
filter_3 = filter_1.T
filter_4 = -filter_3
filters = np.array([filter_1, filter_2, filter_3, filter_4])
# visualizing filters
fig = plt.figure(figsize=(10, 5))
for i in range(4):
    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])
    ax.imshow(filters[i], cmap='gray')
    ax.set_title('Filter %s' % str(i+1))
    width, height = filters[i].shape
    for x in range(width):
        for y in range(height):
            ax.annotate(str(filters[i][x][y]), xy=(y,x),
                        horizontalalignment='center',
                        verticalalignment='center',
                        color='white' if filters[i][x][y]<0 else 'black')
                        
                        
                        
```



<img src="/images/cv/39.png">


 
Pytorch for Deep Learning

```
import torch
import torch.nn as nn
import torch.nn.functional as F

# data loading and transforming
from torchvision.datasets import FashionMNIST
from torch.utils.data import DataLoader
from torchvision import transforms
```

Convolutional layer
Single convolutional layer ch·ª©a t·∫•t c·∫£ c√°c b·ªô l·ªçc ƒë√£ t·∫°o. Kh·ªüi t·∫°o c√°c tr·ªçng s·ªë trong m·ªôt l·ªõp ph·ª©c h·ª£p ƒë·ªÉ c√≥ th·ªÉ h√¨nh dung nh·ªØng g√¨ x·∫£y ra sau khi chuy·ªÉn ti·∫øp qua m·∫°ng n√†y!

```
# neural network with a single convolutional layer with four filters
class Net(nn.Module):
    
    def __init__(self, weight):
        super(Net, self).__init__()
        # initializing the weights of the convolutional layer to be the weights of the 4 defined filters
        k_height, k_width = weight.shape[2:]
        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)
        self.conv.weight = torch.nn.Parameter(weight)

    def forward(self, x):
        # calculates the output of a convolutional layer
        # pre- and post-activation
        conv_x = self.conv(x)
        activated_x = F.relu(conv_x)
        
        # returns both layers
        return conv_x, activated_x
    
# instantiating the model and setting the weights
weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)
model = Net(weight)

print(model)

>>> Net((conv): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), bias=False))

# helper function for visualizing the output of a given layer
# default number of filters is 4

def viz_layer(layer, n_filters= 4):
    fig = plt.figure(figsize=(20, 20))
    
    for i in range(n_filters):
        ax = fig.add_subplot(1, n_filters, i+1, xticks=[], yticks=[])
        # grab layer outputs
        ax.imshow(np.squeeze(layer[0,i].data.numpy()), cmap='gray')
        ax.set_title('Output %s' % str(i+1))
# plotting original image
plt.imshow(gray_img, cmap='gray')

# visualizing all filters
fig = plt.figure(figsize=(12, 6))
fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)
for i in range(4):
    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])
    ax.imshow(filters[i], cmap='gray')
    ax.set_title('Filter %s' % str(i+1))

# converting the image into an input Tensor
gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)

# getting the convolutional layer (pre and post activation)
conv_layer, activated_layer = model(gray_img_tensor)

# visualizing the output of a conv layer
viz_layer(conv_layer)
```



<img src="/images/cv/40.png">
<img src="/images/cv/41.png">
<img src="/images/cv/42.png">
 
#### Pooling Layer
Pooling Layer cung c·∫•p m·ªôt c√°ch ti·∫øp c·∫≠n ƒë·ªÉ down sampling feature maps b·∫±ng c√°ch t√≥m t·∫Øt s·ª± hi·ªán di·ªán c·ªßa feature maps trong patchs c·ªßa feature maps. Hai ph∆∞∆°ng ph√°p t·ªïng h·ª£p ph·ªï bi·∫øn l√† pooling v√† max pooling t√≥m t·∫Øt s·ª± hi·ªán di·ªán trung b√¨nh c·ªßa m·ªôt feature v√† s·ª± hi·ªán di·ªán ƒë∆∞·ª£c k√≠ch ho·∫°t nhi·ªÅu nh·∫•t c·ªßa m·ªôt feature t∆∞∆°ng ·ª©ng.
```
# Adding a pooling layer of size (4, 4)
class Net(nn.Module):
    
    def __init__(self, weight):
        super(Net, self).__init__()
        k_height, k_width = weight.shape[2:]
        self.conv = nn.Conv2d(1, 4, kernel_size=(k_height, k_width), bias=False)
        self.conv.weight = torch.nn.Parameter(weight)
        # defining a pooling layer
        self.pool = nn.MaxPool2d(4, 4)

    def forward(self, x):
        # calculates the output of a convolutional layer
        # pre- and post-activation
        conv_x = self.conv(x)
        activated_x = F.relu(conv_x)
        
        # applies pooling layer
        pooled_x = self.pool(activated_x)
        
        # returns all layers
        return conv_x, activated_x, pooled_x
    
weight = torch.from_numpy(filters).unsqueeze(1).type(torch.FloatTensor)
model = Net(weight)

print(model)

>>>
Net((conv): Conv2d(1, 4, kernel_size=(4, 4), stride=(1, 1), bias=False)
(pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=Fase)


plt.imshow(gray_img, cmap='gray')

# visualizing
fig = plt.figure(figsize=(12, 6))
fig.subplots_adjust(left=0, right=1.5, bottom=0.8, top=1, hspace=0.05, wspace=0.05)
for i in range(4):
    ax = fig.add_subplot(1, 4, i+1, xticks=[], yticks=[])
    ax.imshow(filters[i], cmap='gray')
    ax.set_title('Filter %s' % str(i+1))

    
# chuy·ªÉn data th√†nh tensor
gray_img_tensor = torch.from_numpy(gray_img).unsqueeze(0).unsqueeze(1)

conv_layer, activated_layer, pooled_layer = model(gray_img_tensor)

# visualizing the output of the activated conv layer
viz_layer(activated_layer)

# visualizing the output of the pooling layer
viz_layer(pooled_layer)

```


<img src="/images/cv/43.png">
<img src="/images/cv/44.png">
<img src="/images/cv/45.png">
<img src="/images/cv/46.png">


### P/S
Trong khu√¥n kh·ªï b√†i vi·∫øt n√†y, m√¨nh ƒë√£ ƒë·ªÅ c·∫≠p t·ªõi m·ªôt s·ªë ph∆∞∆°ng ph√°p th∆∞·ªùng s·ª≠ d·ª•ng trong vi·ªác x·ª≠ l√Ω ·∫£nh. OpenCV l√† m·ªôt th∆∞ vi·ªán r·∫•t m·∫°nh h·ªó tr·ª£ c√°c h√†m x·ª≠ l√Ω ·∫£nh. Vi·ªác x·ª≠ d·ª•ng th√†nh th·∫°o OpenCV s·∫Ω l√† m·ªôt l·ª£i th·∫ø m·∫°nh trong vi·ªác x·ª≠ l√Ω ·∫£nh v√† ti·ªÅn x·ª≠ l√Ω data raw cho c√°c m√¥ h√¨nh M√°y h·ªçc c≈©ng nh∆∞ c√°c m√¥ h√¨nh h·ªçc s√¢u.<br>
M√¨nh v·ª´a k·∫øt th√∫c ph·∫ßn 1. Ph·∫ßn 2 (t·ª´ CNN t·ªõi SOTA) m√¨nh s·∫Ω c·ªë g·∫Øng d√†nh th·ªùi gian ƒë·ªÉ vi·∫øt v·ªÅ n√≥ m·ªôt c√°ch ng·∫Øn v√† d·ªÖ hi·ªÉu nh·∫•t (m√¨nh c≈©ng ch∆∞a bi·∫øt khi n√†o xong v√¨ n√≥ th·ª±c s·ª± qu√° nhi·ªÅu v√† qu√° d√†i üôÅ). Computer Vision l√† m·ªôt lƒ©nh v·ª±c r·∫•t r·∫•t l·ªõn, trong khu√¥n kh·ªï 1, 2 b√†i vi·∫øt kh√¥ng th·ªÉ ho√†n to√†n bao ph·ªß h·∫øt v·ªÅ n√≥, ch·ªâ mong qua b√†i vi·∫øt c·ªßa m√¨nh, c√°c b·∫°n c√≥ th√™m nhi·ªÅu ƒë·ªông l·ª±c ƒë·ªÉ t√¨m hi·ªÉu v·ªÅ th·ªã gi√°c m√°y t√≠nh.<br>
C√≥ th·ªÉ trong qu√° tr√¨nh vi·∫øt c√≥ sai s√≥t, hi m·ªçi ng∆∞·ªùi c√πng s·ª≠a ch·ªØa ƒë·ªÉ m·ªçi th·ª© t·ªët h∆°n.
<div class="gallery-box">
  <div class="gallery">
    <img src="/images/admin/minh.jpg">
  </div>
  <em>Computer Vision/ <a href="https://fb.com/itmobrain" target="_blank">ITMO Brain</a></em>
</div>
><cite>Thanks for reading. <br>
Minh!</cite


#### Useful links/Books: 
[Computer Vision: Algorithms and Applications (Texts in Computer Science): Szeliski, Richard ](https://www.amazon.com/Computer-Vision-Algorithms-Applications-Science/dp/1848829345)<br>
(highly recommend v√¨ cu·ªën s√°ch n√†y tuy c·ª±c k·ª≥ h√†n l√¢m nh∆∞ng n√≥ l√† cu·ªën s√°ch r·∫•t r·∫•t hay)
 
[Rapid Object Detection using a Boosted Cascade of Simple Features ](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf)<br>
[OpenCV Python Tutorial - GeeksforGeeks](https://www.geeksforgeeks.org/opencv-python-tutorial/)<br>
[15 OpenCV Projects Ideas for Beginners to Practice in 2021 (projectpro.io)](https://www.projectpro.io/article/opencv-projects-ideas-/492)<br>
[Start Here with Computer Vision, Deep Learning, and OpenCV - PyImageSearch](https://www.pyimagesearch.com/start-here/)<br>
[CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/convolutional-networks/)

